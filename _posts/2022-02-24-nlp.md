---
layout: post
title: Natural Language Processing 
subtitle: Self study using Huggingface
cover-img: /assets/img/bert.png
thumbnail-img: /assets/img/HF.png
tags: [Self Study, Deep Learning]
comments: true
---

Natural Language Processing (NLP) is the subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data *[Wikipedia](https://en.wikipedia.org/wiki/Natural_language_processing)*. 

Nowadays, text data is taking a huge portion in a source of data. For instance, language model like **RoBERTa** can consist of datasets trained with ~58 million Twitter data [source](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment) after the pre-processed text data goes through word embedding to be vectorized. On top of that, I was intrigued by how this new chapter of Deep Learning can accelerate data mining and model training.  

# 1. NLP Basics

I worked on the Notion page to organize the fundamental concepts. [Notion](https://sunbinmun.notion.site/NLP-Tasks-e75c354d99d349799de62a3ec81c5f0f)

All references for the image are from Google. All materials are in mix of English ðŸ‡ºðŸ‡¸ and Korean ðŸ‡°ðŸ‡·

# 2. HuggingFace

## 1. Code Interpretation

Huggingface (&copy;) is the single-most famous language model hub that users can build, train and deploy state of the art models powered by the reference open source in machine learning. You can search for models based on the **task**(Image Classification, Translation, Image Segmentation, Fill-Mask, Automatic Speech Recognition, Sentence Similarity, Audio Classification), **library** (PyTorch, Tensorflow...), and **dataset**. 

I followed a tutorial presented by Huggingface (&copy;). The following repository is my attempt to keep record [Github](https://github.com/msb1002/HuggingFace)





